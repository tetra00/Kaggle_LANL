{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC IDEA OF THE KERNEL\n",
    "\n",
    "The data consists of a one dimensional time series x with 600 Mio data points. \n",
    "\n",
    "At test time, we will see a time series of length 150'000 to predict the next earthquake.\n",
    "\n",
    "The idea of this kernel is to randomly sample chunks of length 150'000 from x, \n",
    "\n",
    "derive some features and use them to update weights of a recurrent neural net with 150'000 / 1000 = 150 time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fix seeds\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "#float_data = pd.read_csv(\"../input/train.csv\", dtype={\"acoustic_data\": np.float32, \"time_to_failure\": np.float32}).values\n",
    "df_train = pd.read_hdf(\"../input/train.hdf\", key='0')\n",
    "float_data = df_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for the data generator. Extracts mean, standard deviation, and quantiles per time step.\n",
    "# Can easily be extended. Expects a two dimensional array.\n",
    "def extract_features(z):\n",
    "     return np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given ending position \"last_index\", we split the last 150'000 values \n",
    "# of \"x\" into 150 pieces of length 1000 each. So n_steps * step_length should equal 150'000.\n",
    "# From each piece, a set features are extracted. This results in a feature matrix \n",
    "# of dimension (150 time steps x features).  \n",
    "def create_X(x, last_index=None, n_steps=150, step_length=1000):\n",
    "    if last_index == None:\n",
    "        last_index=len(x)\n",
    "       \n",
    "    assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "    # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "    temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
    "    \n",
    "    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
    "    # of the last 10 observations. \n",
    "    return np.c_[extract_features(temp),\n",
    "                 extract_features(temp[:, -step_length // 10:]),\n",
    "                 extract_features(temp[:, -step_length // 100:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our RNN is based on 12 features\n"
     ]
    }
   ],
   "source": [
    "# Query \"create_X\" to figure out the number of features\n",
    "X = create_X(float_data[0:150000])\n",
    "n_features = X.shape[1]\n",
    "print(\"Our RNN is based on %i features\"% n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "def generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1\n",
    "     \n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            targets[j] = data[row - 1, 1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006954822"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Position of second (of 16) earthquake. Used to have a clean split\n",
    "# between train and validation\n",
    "second_earthquake = 50085877\n",
    "float_data[second_earthquake, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = float_data \n",
    "\n",
    "batch_size=16\n",
    "n_steps=150\n",
    "step_length=1000\n",
    "batch_size=batch_size\n",
    "min_index=second_earthquake + 1\n",
    "max_index = len(data) - 1\n",
    "\n",
    "# Pick indices of ending positions\n",
    "rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "\n",
    "# Initialize feature matrices and targets\n",
    "samples = np.zeros((batch_size, n_steps, n_features))\n",
    "targets = np.zeros(batch_size, )\n",
    "\n",
    "for j, row in enumerate(rows):\n",
    "    samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "    targets[j] = data[row - 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generators\n",
    "batch_size = 32\n",
    "\n",
    "#train_gen = generator(float_data, batch_size=batch_size) # Use this for better score\n",
    "train_gen = generator(float_data, batch_size=batch_size, min_index=second_earthquake + 1)\n",
    "valid_gen = generator(float_data, batch_size=batch_size, max_index=second_earthquake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnngru_2 (CuDNNGRU)       (None, 48)                8928      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                490       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,429\n",
      "Trainable params: 9,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "cb = [\n",
    "    ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3),\n",
    "    #TensorBoard(log_dir=\"tflog/\", histogram_freq=1)\n",
    "]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNGRU(48, input_shape=(None, n_features)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 30s - loss: 2.0844 - val_loss: 1.7549\n",
      "Epoch 2/30\n",
      " - 29s - loss: 2.0863 - val_loss: 1.6739\n",
      "Epoch 3/30\n",
      " - 30s - loss: 2.0929 - val_loss: 1.7704\n",
      "Epoch 4/30\n",
      " - 34s - loss: 2.0835 - val_loss: 1.8700\n",
      "Epoch 5/30\n",
      " - 34s - loss: 2.0768 - val_loss: 1.6996\n",
      "Epoch 6/30\n",
      " - 32s - loss: 2.0630 - val_loss: 1.8198\n",
      "Epoch 7/30\n",
      " - 32s - loss: 2.0776 - val_loss: 1.8230\n",
      "Epoch 8/30\n",
      " - 32s - loss: 2.0831 - val_loss: 1.7619\n",
      "Epoch 9/30\n",
      " - 32s - loss: 2.0827 - val_loss: 1.8405\n",
      "Epoch 10/30\n",
      " - 32s - loss: 2.0630 - val_loss: 1.6870\n",
      "Epoch 11/30\n",
      " - 32s - loss: 2.0531 - val_loss: 1.6264\n",
      "Epoch 12/30\n",
      " - 32s - loss: 2.0445 - val_loss: 1.5964\n",
      "Epoch 13/30\n",
      " - 32s - loss: 2.0432 - val_loss: 1.6392\n",
      "Epoch 14/30\n",
      " - 32s - loss: 2.0435 - val_loss: 1.6984\n",
      "Epoch 15/30\n",
      " - 32s - loss: 2.0333 - val_loss: 1.6971\n",
      "Epoch 16/30\n",
      " - 32s - loss: 2.0392 - val_loss: 1.5970\n",
      "Epoch 17/30\n",
      " - 32s - loss: 2.0390 - val_loss: 1.7006\n",
      "Epoch 18/30\n",
      " - 33s - loss: 2.0451 - val_loss: 1.5917\n",
      "Epoch 19/30\n",
      " - 33s - loss: 2.0241 - val_loss: 1.7006\n",
      "Epoch 20/30\n",
      " - 31s - loss: 2.0309 - val_loss: 1.6529\n",
      "Epoch 21/30\n",
      " - 32s - loss: 2.0213 - val_loss: 1.7282\n",
      "Epoch 22/30\n",
      " - 31s - loss: 2.0268 - val_loss: 1.7994\n",
      "Epoch 23/30\n",
      " - 32s - loss: 2.0281 - val_loss: 1.5420\n",
      "Epoch 24/30\n",
      " - 31s - loss: 2.0163 - val_loss: 1.7481\n",
      "Epoch 25/30\n",
      " - 32s - loss: 2.0203 - val_loss: 1.7424\n",
      "Epoch 26/30\n",
      " - 31s - loss: 2.0354 - val_loss: 1.7261\n",
      "Epoch 27/30\n",
      " - 30s - loss: 2.0363 - val_loss: 1.5988\n",
      "Epoch 28/30\n",
      " - 30s - loss: 2.0376 - val_loss: 1.7770\n",
      "Epoch 29/30\n",
      " - 31s - loss: 2.0250 - val_loss: 1.5920\n",
      "Epoch 30/30\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit model\n",
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=30,\n",
    "                              verbose=2,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "perf_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submission file\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n",
    "\n",
    "# Load each test data, create the feature matrix, get numeric prediction\n",
    "for i, seg_id in enumerate(tqdm(submission.index)):\n",
    "  #  print(i)\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    x = seg['acoustic_data'].values\n",
    "    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n",
    "\n",
    "submission.head()\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
