{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC IDEA OF THE KERNEL\n",
    "\n",
    "The data consists of a one dimensional time series x with 600 Mio data points. \n",
    "\n",
    "At test time, we will see a time series of length 150'000 to predict the next earthquake.\n",
    "\n",
    "The idea of this kernel is to randomly sample chunks of length 150'000 from x, \n",
    "\n",
    "derive some features and use them to update weights of a recurrent neural net with 150'000 / 1000 = 150 time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Higgins\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy import stats\n",
    "\n",
    "from scipy.signal import hilbert, hann, convolve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "# Fix seeds\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "#float_data = pd.read_csv(\"../input/train.csv\", dtype={\"acoustic_data\": np.float32, \"time_to_failure\": np.float32}).values\n",
    "df_train = pd.read_hdf(\"../input/train.hdf\", key='0')\n",
    "\n",
    "float_data = df_train.values\n",
    "acoustic_data = df_train['acoustic_data'].values\n",
    "time_to_failures = df_train['time_to_failure'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    \"\"\"\n",
    "    STA/LTA (short-term average/long-term average)\n",
    "    For noise-free seismograms, the maximum value of the numerical derivative of the STA/LTA ratio is close to the time of the first arrival.\n",
    "    (https://en.wikipedia.org/wiki/First_break_picking)\n",
    "    length_staだけズラした波形の振幅の累積二乗の比率/length_finだけズラした波形の振幅の累積二乗の比率\n",
    "    \"\"\"\n",
    "    sta = np.cumsum(x ** 2)\n",
    "\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    sta[:length_lta - 1] = 0 # Pad zeros\n",
    "    \n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "\n",
    "    return sta / lta\n",
    "\n",
    "def max_abs_diff_calssic_sta_lat(x, length_sta, length_lta):\n",
    "    \"\"\"\n",
    "    x:1D-array(m*1)\n",
    "    return:float\n",
    "    \"\"\"\n",
    "    csl = classic_sta_lta(x, length_sta, length_lta)\n",
    "    return np.abs(np.diff(csl)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend_feature(mat, abs_values=False):\n",
    "    \"\"\"\n",
    "    単回帰を用いてデータの平均傾きを出力\n",
    "    \"\"\"\n",
    "    trends = np.zeros([mat.shape[0],1])\n",
    "    for i in range(mat.shape[0]):\n",
    "        arr = mat[i]\n",
    "        idx = np.array(range(len(arr)))\n",
    "        if abs_values:\n",
    "            arr = np.abs(arr)\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(idx.reshape(-1, 1), arr)\n",
    "        trends[i] = lr.coef_[0]\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for the data generator. Extracts mean, standard deviation, and quantiles per time step.\n",
    "# Can easily be extended. Expects a two dimensional array.\n",
    "def extract_features(z):\n",
    "    \"\"\"\n",
    "    z:2D-array(m*n)\n",
    "    return:features(m*x)\n",
    "    \"\"\"\n",
    "    return np.c_[z.mean(axis=1), \n",
    "                 z.min(axis=1),\n",
    "                 z.max(axis=1),\n",
    "                 z.std(axis=1),\n",
    "                  \n",
    "                 add_trend_feature(z),\n",
    "                 add_trend_feature(z, abs_values=True),                  \n",
    "                 np.abs(z).mean(axis=1),\n",
    "                 np.abs(z).std(axis=1),\n",
    "                  \n",
    "                 #X_out.loc[i, 'mad'] = x_in.mad()\n",
    "                 np.median(z, axis=1).reshape(-1,1),\n",
    "                 stats.kurtosis(z, axis=1).reshape(-1,1),\n",
    "                 stats.skew(z, axis=1).reshape(-1,1),\n",
    "\n",
    "                 #np.abs(hilbert(x_in)).mean()\n",
    "                 #convolve(x_in, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given ending position \"last_index\", we split the last 150'000 values \n",
    "# of \"x\" into 150 pieces of length 1000 each. So n_steps * step_length should equal 150'000.\n",
    "# From each piece, a set features are extracted. This results in a feature matrix \n",
    "# of dimension (150 time steps x features).  \n",
    "def create_X(x, last_index=None, n_steps=150, step_length=1000):\n",
    "    if last_index == None:\n",
    "        last_index=len(x)\n",
    "       \n",
    "    assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "    # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "    temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
    "    \n",
    "    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
    "    # of the last 10 observations. \n",
    "    return np.c_[extract_features(temp),\n",
    "                 extract_features(temp[:, -step_length // 10:]),\n",
    "                 extract_features(temp[:, -step_length // 100:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def extract_features_df(z, col_head=None):\n",
    "#     \"\"\"\n",
    "#     z:2D-array(m*n)\n",
    "#     return 2D-array(m * num_of_feature)\n",
    "#     \"\"\"\n",
    "#     df_ext = pd.DataFrame(index=range(z.shape[0]))\n",
    "    \n",
    "#     df_ext['mean'] = z.mean(axis=1).reshape(-1,1)            \n",
    "#     df_ext['min'] = z.min(axis=1).reshape(-1,1)             \n",
    "#     df_ext['max'] = z.max(axis=1).reshape(-1,1)            \n",
    "#     df_ext['std'] = z.std(axis=1).reshape(-1,1)\n",
    "#    # df_ext['median'] = np.median(z, axis=1).reshape(-1,1)\n",
    "#     df_ext['kurtosis'] = stats.kurtosis(z, axis=1).reshape(-1,1)\n",
    "#     df_ext['skew'] = stats.skew(z, axis=1).reshape(-1,1)\n",
    "\n",
    "#     for i in range(z.shape[0]): \n",
    "#         zs_i = pd.Series(z[i])   \n",
    "        \n",
    "#         if z.shape[1] >= 100:\n",
    "#             #df_ext.loc[i, 'mad_stalat_10-50'] = max_abs_diff_calssic_sta_lat(zs_i, length_sta=10, length_lta=50)\n",
    "#             #df_ext.loc[i, 'mad_stalat_33-66'] = max_abs_diff_calssic_sta_lat(zs_i, length_sta=33, length_lta=66)\n",
    "#             df_ext.loc[i, 'mad_stalat_33-99'] = max_abs_diff_calssic_sta_lat(zs_i, length_sta=33, length_lta=99)\n",
    "#         if z.shape[1] >= 150:\n",
    "#             #df_ext.loc[i, 'mad_stalat_10-100'] = max_abs_diff_calssic_sta_lat(zs_i, length_sta=10, length_lta=100)\n",
    "#             # df_ext.loc[i, 'mad_stalat_33-123'] = max_abs_diff_calssic_sta_lat(zs_i, length_sta=33, length_lta=123)\n",
    "#             df_ext.loc[i, 'mad_stalat_40-100'] = max_abs_diff_calssic_sta_lat(zs_i, length_sta=40, length_lta=100)\n",
    "#             #df_ext.loc[i, 'mad_stalat_50-150'] =  max_abs_diff_calssic_sta_lat(zs_i, length_sta=50, length_lta=150)\n",
    "        \n",
    "#         # Windowを変化させrolling特徴量を生成\n",
    "#         for window in [10]: #,30]:\n",
    "#             if z.shape[1] < window+10:\n",
    "#                 continue    \n",
    "#             z_roll_std = zs_i.rolling(window).std().dropna().values\n",
    "#             z_roll_mean = zs_i.rolling(window).mean().dropna().values   \n",
    "      \n",
    "#             df_ext.loc[i, 'ave_roll_std_wd_' + str(window)] = z_roll_std.mean()          \n",
    "#             df_ext.loc[i, 'std_roll_std_wd_' + str(window)] = z_roll_std.std()\n",
    "# #             df_ext.loc[i, 'max_roll_std_wd_' + str(window)] = z_roll_std.max()\n",
    "# #             df_ext.loc[i, 'min_roll_std_wd_' + str(window)] = z_roll_std.min()\n",
    "#             df_ext.loc[i, 'ave_roll_mean_wd_' + str(window)] = z_roll_mean.mean()\n",
    "#             df_ext.loc[i, 'std_roll_mean_wd_' + str(window)] = z_roll_mean.std()\n",
    "# #             df_ext.loc[i, 'max_roll_mean_wd_' + str(window)] = z_roll_mean.max()\n",
    "# #             df_ext.loc[i, 'min_roll_mean_wd_' + str(window)] = z_roll_mean.min()\n",
    "    \n",
    "#     df_ext.dropna(axis=1, inplace=True)\n",
    "    \n",
    "#     if col_head is not None:\n",
    "#         df_ext.columns = [str(col_head) + col for col in df_ext.columns]\n",
    "    \n",
    "#     return df_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For a given ending position \"last_index\", we split the last 150'000 values \n",
    "# # of \"x\" into 150 pieces of length 1000 each. So n_steps * step_length should equal 150'000.\n",
    "# # From each piece, a set features are extracted. This results in a feature matrix \n",
    "# # of dimension (150 time steps x features).  \n",
    "# def create_feature_df(x, last_index=None, n_steps=150, step_length=1000):\n",
    "#     if last_index == None:\n",
    "#         last_index=len(x)\n",
    "       \n",
    "#     assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "#     # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "#     #temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
    "#     z = x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1)\n",
    "    \n",
    "#     # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
    "#     # of the last 10 observations. \n",
    "# #     return np.c_[extract_features(temp),\n",
    "# #                  extract_features(temp[:, -step_length // 10:]),\n",
    "# #                  extract_features(temp[:, -step_length // 100:])]\n",
    "#     return pd.concat([extract_features_df(z, 'all_'), \n",
    "#                       extract_features_df(z[:, -step_length // 10:], 'last100_'), \n",
    "#                       extract_features_df(z[:, -step_length // 100:], 'last10_')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our RNN is based on 33 features\n"
     ]
    }
   ],
   "source": [
    "# Query \"create_X\" to figure out the number of features\n",
    "#df_feat_tr = create_feature_df(acoustic_data[0:150000])\n",
    "#n_features = df_feat_tr.shape[1]\n",
    "data_tr = create_X(float_data[0:150000])\n",
    "n_features = data_tr.shape[1]\n",
    "print(\"Our RNN is based on %i features\"% n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "def generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1\n",
    "        \n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            targets[j] = data[row - 1, 1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# # the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "# def generator_parallel(data, min_index=0, max_index=None, batch_size=10000, n_steps=150, step_length=1000):\n",
    "#     if max_index is None:\n",
    "#         max_index = len(data) - 1\n",
    "        \n",
    "#     #count = 1\n",
    "#     while True:\n",
    "#         #print('Generate:', count)\n",
    "#         #count += 1\n",
    "        \n",
    "#         # Pick indices of ending positions\n",
    "#         rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "         \n",
    "#         def process(last_index):\n",
    "#             # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "#             z = data[(last_index - n_steps * step_length):last_index,0].reshape(n_steps, -1)\n",
    "\n",
    "#             # Extracts features of sequences of full length 1000, of the last 100 & 10 observations. \n",
    "#             df_feat_all = extract_features_df(z, 'all_')\n",
    "#             df_feat_last100 = extract_features_df(z[:, -step_length // 10:], 'last100_')\n",
    "#             df_feat_last10 = extract_features_df(z[:, -step_length // 100:], 'last10_')\n",
    "#             features = np.c_[df_feat_all.values, df_feat_last100.values, df_feat_last10.values]\n",
    "#             # create_feature_df(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length).values\n",
    "#             return features\n",
    "\n",
    "#         samples = np.array(Parallel(n_jobs=-1)([delayed(process)(r) for r in rows]))\n",
    "#         targets = np.array([data[r - 1, 1] for r in rows])\n",
    "        \n",
    "#         yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# # the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "# def generate_data(data, size, min_index=0, max_index=None,  n_steps=150, step_length=1000):\n",
    "#     if max_index is None:\n",
    "#         max_index = len(data) - 1\n",
    "        \n",
    "#     # Pick indices of ending positions\n",
    "#     rows = np.random.randint(min_index + n_steps * step_length, max_index, size=size)\n",
    "\n",
    "#     # Initialize feature matrices and targets\n",
    "#     samples = np.zeros((size, n_steps, n_features))\n",
    "#     targets = np.zeros(size, )\n",
    "    \n",
    "#     def process(last_index):\n",
    "#         # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "#         z = data[(last_index - n_steps * step_length):last_index,0].reshape(n_steps, -1)\n",
    "\n",
    "#         # Extracts features of sequences of full length 1000, of the last 100 & 10 observations. \n",
    "#         df_feat_all = extract_features_df(z, 'all_')\n",
    "#         df_feat_last100 = extract_features_df(z[:, -step_length // 10:], 'last100_')\n",
    "#         df_feat_last10 = extract_features_df(z[:, -step_length // 100:], 'last10_')\n",
    "#         features = np.c_[df_feat_all.values, df_feat_last100.values, df_feat_last10.values]\n",
    "\n",
    "#         return features\n",
    "    \n",
    "#     # 100サンプルごとに更新\n",
    "#     update_freq = 100\n",
    "#     for i in tqdm_notebook(range(size//update_freq)):\n",
    "#         j_min = i*update_freq \n",
    "#         j_max = np.min([size, (i+1)*update_freq])\n",
    "#         rows_i = rows[j_min:j_max]\n",
    "#         samples[j_min:j_max] = np.array(Parallel(n_jobs=-1)([delayed(process)(r) for r in rows_i]))\n",
    "#         targets[j_min:j_max] = np.array([data[r - 1, 1] for r in rows_i])\n",
    "        \n",
    "#     return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position of second (of 16) earthquake. Used to have a clean split\n",
    "# between train and validation\n",
    "second_earthquake = 50085877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generators\n",
    "batch_size = 32\n",
    "\n",
    "# #train_gen = generator(data=df_train.values, batch_size=batch_size) # Use this for better score\n",
    "train_gen = generator(float_data, batch_size=batch_size, min_index=second_earthquake + 1)\n",
    "valid_gen = generator(data=df_train.values, batch_size=batch_size, max_index=second_earthquake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52d948905fa4a3c9bebfebef6f0aa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x_train, y_train = generate_data(float_data, size=1000, min_index=second_earthquake + 1)\n",
    "# x_valid, y_valid = generate_data(float_data, size=100, max_index=second_earthquake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "from keras import activations, regularizers, initializers, constraints\n",
    "from keras.engine import Layer, InputSpec\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is NN LSTM Model creation\n",
    "def model_lstm(input_shape, feat_shape):\n",
    "    inp = Input(shape=(input_shape[1], input_shape[2],))\n",
    "    feat = Input(shape=(feat_shape[1],))\n",
    "\n",
    "    bi_lstm = Bidirectional(CuDNNLSTM(128, return_sequences=True), merge_mode='concat')(inp)\n",
    "    bi_gru = Bidirectional(CuDNNGRU(64, return_sequences=True), merge_mode='concat')(bi_lstm)\n",
    "    \n",
    "    attention = Attention(input_shape[1])(bi_gru)\n",
    "    \n",
    "    x = concatenate([attention, feat], axis=1)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[inp, feat], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnngru_3 (CuDNNGRU)       (None, 48)                11952     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                490       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 12,453\n",
      "Trainable params: 12,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU, Bidirectional\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "cb = [\n",
    "    ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3),\n",
    "    #TensorBoard(log_dir=\"tflog/\", histogram_freq=1)\n",
    "]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNGRU(48, input_shape=(None, n_features)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Fail to find the dnn implementation.\n\t [[{{node cu_dnngru_3/CudnnRNN}}]]\n\t [[{{node loss_3/mul}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-b3289844c333>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                               validation_steps=200)\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# history = model.fit(x_train, y_train,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Fail to find the dnn implementation.\n\t [[{{node cu_dnngru_3/CudnnRNN}}]]\n\t [[{{node loss_3/mul}}]]"
     ]
    }
   ],
   "source": [
    "# Compile and fit model\n",
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=128,\n",
    "                              epochs=50,\n",
    "                              verbose=1,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200)\n",
    "\n",
    "# history = model.fit(x_train, y_train,\n",
    "#                     batch_size=1000,\n",
    "#                     epochs=1000,\n",
    "#                     verbose=2,\n",
    "#                     callbacks=cb,\n",
    "#                     validation_data=(x_valid, y_valid),\n",
    "#                     #validation_steps=200\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "perf_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submission file\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n",
    "\n",
    "# Load each test data, create the feature matrix, get numeric prediction\n",
    "for i, seg_id in enumerate(tqdm(submission.index)):\n",
    "  #  print(i)\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    x = seg['acoustic_data'].values\n",
    "    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n",
    "\n",
    "submission.head()\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
