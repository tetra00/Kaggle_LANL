{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC IDEA OF THE KERNEL\n",
    "\n",
    "The data consists of a one dimensional time series x with 600 Mio data points. \n",
    "\n",
    "At test time, we will see a time series of length 150'000 to predict the next earthquake.\n",
    "\n",
    "The idea of this kernel is to randomly sample chunks of length 150'000 from x, \n",
    "\n",
    "derive some features and use them to update weights of a recurrent neural net with 150'000 / 1000 = 150 time steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fix seeds\n",
    "from numpy.random import seed\n",
    "seed(639)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "#float_data = pd.read_csv(\"../input/train.csv\", dtype={\"acoustic_data\": np.float32, \"time_to_failure\": np.float32}).values\n",
    "df_train = pd.read_hdf(\"../input/train.hdf\", key='0')\n",
    "float_data = df_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for the data generator. Extracts mean, standard deviation, and quantiles per time step.\n",
    "# Can easily be extended. Expects a two dimensional array.\n",
    "def extract_features(z):\n",
    "     return np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given ending position \"last_index\", we split the last 150'000 values \n",
    "# of \"x\" into 150 pieces of length 1000 each. So n_steps * step_length should equal 150'000.\n",
    "# From each piece, a set features are extracted. This results in a feature matrix \n",
    "# of dimension (150 time steps x features).  \n",
    "def create_X(x, last_index=None, n_steps=150, step_length=1000):\n",
    "    if last_index == None:\n",
    "        last_index=len(x)\n",
    "       \n",
    "    assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "    # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "    temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
    "    \n",
    "    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
    "    # of the last 10 observations. \n",
    "    return np.c_[extract_features(temp),\n",
    "                 extract_features(temp[:, -step_length // 10:]),\n",
    "                 extract_features(temp[:, -step_length // 100:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our RNN is based on 12 features\n"
     ]
    }
   ],
   "source": [
    "# Query \"create_X\" to figure out the number of features\n",
    "X = create_X(float_data[0:150000])\n",
    "n_features = X.shape[1]\n",
    "print(\"Our RNN is based on %i features\"% n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generator endlessly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "def generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1\n",
    "     \n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            targets[j] = data[row - 1, 1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position of second (of 16) earthquake. Used to have a clean split\n",
    "# between train and validation\n",
    "second_earthquake = 50085877\n",
    "#float_data[second_earthquake, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generators\n",
    "batch_size = 32\n",
    "\n",
    "#train_gen = generator(float_data, batch_size=batch_size) # Use this for better score\n",
    "train_gen = generator(float_data, batch_size=batch_size, min_index=second_earthquake + 1)\n",
    "valid_gen = generator(float_data, batch_size=batch_size, max_index=second_earthquake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU, CuDNNLSTM, GRU, Bidirectional\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [\n",
    "    ModelCheckpoint(\"model.hdf5\", save_best_only=True, period=3),\n",
    "    #TensorBoard(log_dir=\"tflog/\", histogram_freq=1)\n",
    "]\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(CuDNNLSTM(48, input_shape=(None, n_features)))\n",
    "model.add(CuDNNLSTM(48, input_shape=(None, n_features)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_10 (CuDNNLSTM)    (None, 48)                11904     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                490       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 12,405\n",
      "Trainable params: 12,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit model\n",
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 2.3977 - val_loss: 1.8171\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 2.1677 - val_loss: 2.0083\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 36s 36ms/step - loss: 2.1506 - val_loss: 2.0260\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 35s 35ms/step - loss: 2.1149 - val_loss: 1.8496\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 36s 36ms/step - loss: 2.1175 - val_loss: 1.6625\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 36s 36ms/step - loss: 2.1117 - val_loss: 1.9814\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 2.0919 - val_loss: 1.8502\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 2.0906 - val_loss: 1.8285\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 2.0940 - val_loss: 1.9028\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 2.0956 - val_loss: 1.7957\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 2.0818 - val_loss: 1.8153\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 2.0683 - val_loss: 1.8096\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 37s 37ms/step - loss: 2.0872 - val_loss: 1.6927\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 38s 38ms/step - loss: 2.0789 - val_loss: 1.8536\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 39s 39ms/step - loss: 2.0612 - val_loss: 1.9148\n",
      "Epoch 16/30\n",
      " 998/1000 [============================>.] - ETA: 0s - loss: 2.0594"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=30,\n",
    "                              verbose=1,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "perf_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submission file\n",
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n",
    "\n",
    "# Load each test data, create the feature matrix, get numeric prediction\n",
    "for i, seg_id in enumerate(tqdm(submission.index)):\n",
    "  #  print(i)\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    x = seg['acoustic_data'].values\n",
    "    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n",
    "\n",
    "submission.head()\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
